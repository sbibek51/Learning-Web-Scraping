{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c836144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b51142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibek Shiwakoti\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7941c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    \"User-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
    "\n",
    "# Skills and Place of Work\n",
    "skill = input('Enter your Skill: ').strip()\n",
    "place = input('Enter the location: ').strip()\n",
    "no_of_pages = int(input('Enter the #pages to scrape: '))\n",
    "\n",
    "\n",
    "# Creating the Main Directory\n",
    "main_dir = os.getcwd() + '\\\\'\n",
    "if not os.path.exists(main_dir):\n",
    "    os.mkdir(main_dir)\n",
    "    print('Base Directory Created Successfully.')\n",
    "\n",
    "\n",
    "# Name of the CSV File\n",
    "file_name = skill.title() + '_' + place.title() + '_Jobs.csv'\n",
    "# Path of the CSV File\n",
    "file_path = main_dir + file_name\n",
    "\n",
    "# Writing to the CSV File\n",
    "with open(file_path, mode='w') as file:\n",
    "    writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "    # Adding the Column Names to the CSV File\n",
    "    writer.writerow(\n",
    "        ['JOB_NAME', 'COMPANY', 'LOCATION', 'POSTED', 'APPLY_LINK'])\n",
    "\n",
    "    # Requesting and getting the webpage using requests\n",
    "    print(f'\\nScraping in progress...\\n')\n",
    "    for page in range(no_of_pages):\n",
    "        url = 'https://www.indeed.co.in/jobs?q=' + skill + \\\n",
    "            '&l=' + place + '&start=' + str(page * 10)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        html = response.text\n",
    "\n",
    "        # Scrapping the Web\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        base_url = 'https://in.indeed.com/viewjob?jk='\n",
    "        d = soup.find('div', attrs={'id': 'mosaic-provider-jobcards'})\n",
    "\n",
    "        jobs = soup.find_all('a', class_='tapItem')\n",
    "\n",
    "        for job in jobs:\n",
    "            job_id = job['id'].split('_')[-1]\n",
    "            job_title = job.find('span', title=True).text.strip()\n",
    "            company = job.find('span', class_='companyName').text.strip()\n",
    "            location = job.find('div', class_='companyLocation').text.strip()\n",
    "            posted = job.find('span', class_='date').text.strip()\n",
    "            job_link = base_url + job_id\n",
    "            #print([job_title, company, location, posted, job_link])\n",
    "\n",
    "            # Writing to CSV File\n",
    "            writer.writerow(\n",
    "                [job_title, company, location.title(), posted, job_link])\n",
    "\n",
    "print(f'Jobs data written to <{file_name}> successfully.')  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c0297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf0ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import time\n",
    "\n",
    "def generate_url(jobTitle,location):\n",
    "    \"\"\"Generate URL given job title and location\"\"\"\n",
    "    url = f\"https://ca.indeed.com/{jobTitle}-jobs-in-{location}\"\n",
    "    return url\n",
    "\n",
    "def get_record(card):\n",
    "    \"\"\"Extract info from one job card\"\"\"\n",
    "    \n",
    "    atag = card.h2.a\n",
    "    jobTitle = atag.get('title')\n",
    "    jobURL = \"https://ca.indeed.com\" + atag.get('href')\n",
    "    jobCompany = card.find('span',class_='company').text.strip()\n",
    "    \n",
    "    # for job location, sometimes it's <span> sometimes it's <div> \n",
    "    if card.find('span',class_='location'):\n",
    "        jobLocation = card.find('span',class_='location').text.strip()\n",
    "    else:\n",
    "        jobLocation = card.find('div',class_='location').text.strip()\n",
    "        \n",
    "    jobSummary = card.find('div','summary').text.strip()\n",
    "    jobPostDate = card.find('span','date').text\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # sometimes there's salary\n",
    "    if card.find('span','salaryText'):\n",
    "        jobSalary = card.find('span','salaryText').text.strip() \n",
    "    else:\n",
    "        jobSalary = ''\n",
    "        \n",
    "    job = (jobTitle,jobCompany,jobLocation,jobPostDate,today,jobSalary,jobSummary,jobURL)\n",
    "    \n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf9bf56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ca.indeed.com/python-jobs-in-Canada\n"
     ]
    }
   ],
   "source": [
    "# should maybe rotate\n",
    "user_agent = {'User-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.3 Safari/605.1.15'}\n",
    "\n",
    "records = []\n",
    "\n",
    "job_title = \"python\"  # empty string for all jobs\n",
    "loc = \"Canada\"\n",
    "url = generate_url(jobTitle = job_title, location = loc)\n",
    "print(url)\n",
    "# there will be pop-ups but that doesn't matter\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    response = requests.get(url, user_agent)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    cards = soup.find_all('div',class_='job_seen_beacon')\n",
    "    \n",
    "    for card in cards: \n",
    "        record = get_record(card)\n",
    "        records.append(record)    # append tuple to list\n",
    "        \n",
    "    try:\n",
    "        url = \"https://ca.indeed.com\" + soup.find('a',{'aria-label':'Next'}).get('href')\n",
    "    except AttributeError:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74cfde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobTitle</th>\n",
       "      <th>jobCompany</th>\n",
       "      <th>jobLocation</th>\n",
       "      <th>jobPostDate</th>\n",
       "      <th>today</th>\n",
       "      <th>jobSalary</th>\n",
       "      <th>jobSummary</th>\n",
       "      <th>jobURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [jobTitle, jobCompany, jobLocation, jobPostDate, today, jobSalary, jobSummary, jobURL]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataframe \n",
    "records_df = pd.DataFrame(records, columns =['jobTitle','jobCompany','jobLocation','jobPostDate','today','jobSalary','jobSummary','jobURL']) \n",
    "\n",
    "records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f688d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57ed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe9aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
