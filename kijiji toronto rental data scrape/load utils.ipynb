{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1555dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc0a0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def format_time_elapsed(start_time, elapsed_time):\n",
    "    total_seconds = elapsed_time.total_seconds()\n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "\n",
    "def print_time_info(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    total_seconds = elapsed_time.total_seconds()\n",
    "    hours = int(total_seconds // 3600)\n",
    "    minutes = int((total_seconds % 3600) // 60)\n",
    "    seconds = int(total_seconds % 60)\n",
    "    print(\"Time elapsed:\", hours, \"hours,\", minutes, \"minutes,\", seconds, \"seconds\")\n",
    "\n",
    "\n",
    "def get_last_page(soup):\n",
    "    # Find the element containing pagination\n",
    "    pagination_element = soup.find('ul', {'data-testid': 'pagination-list'})\n",
    "\n",
    "    # Find all page links\n",
    "    page_links = pagination_element.find_all('li', {'data-testid': 'pagination-list-item'})\n",
    "\n",
    "    # Extract the last page number\n",
    "    last_page_number = page_links[-1].get_text(strip=True)\n",
    "\n",
    "    # Extract only the number using regular expressions\n",
    "    last_page_number = re.search(r'\\d+', last_page_number).group()\n",
    "\n",
    "    print(\"Last page number \", last_page_number)\n",
    "\n",
    "    return int(last_page_number)\n",
    "\n",
    "\n",
    "# Function to get the BeautifulSoup object from a URL\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    else:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_section_text(html_soup, string, parent_element, next_element):\n",
    "    # Find the container for the \"Wi-Fi and More\" section\n",
    "    section = html_soup.find(parent_element, string=string)\n",
    "\n",
    "    # Extract the text \"Not Included\" from the Wi-Fi section if it exists\n",
    "    text = ''\n",
    "    if section:\n",
    "        text = section.find_next(next_element).get_text(strip=True)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_multiple_section_text(soup, section_title):\n",
    "    section_heading = soup.find('h4', string=section_title)\n",
    "    text = ''\n",
    "    if section_heading:\n",
    "        section_ul = section_heading.find_next('ul')\n",
    "        if section_ul:\n",
    "            items = section_ul.find_all('li')\n",
    "            text = ', '.join(item.get_text(strip=True) for item in items if item.get_text(strip=True))\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ee5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26199a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
